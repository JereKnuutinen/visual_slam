{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "import os \n",
    "from pathlib import Path\n",
    "from numbers import Number\n",
    "import re\n",
    "from PIL import Image\n",
    "\n",
    "def KeyPoints2CameraCoords(kp, K, w, h):\n",
    "    # Keypoints to homogeneous point coordinates\n",
    "    kp_tf = np.concatenate((kp, np.ones((np.shape(kp)[0],1))), axis=1)\n",
    "    # Keypoints to camera coordinate system\n",
    "    kp_tf = kp_tf @ np.linalg.inv(K.T)\n",
    "    limits = np.squeeze(np.array([w, h, 1]) @ np.linalg.inv(K.T)) # e.g. [1920,1080,1] to camera coords\n",
    "    w_k, h_k = limits[0,0], limits[0,1]\n",
    "    # Define T_norm based on width and height of the image (now in camera coords)\n",
    "    T_norm = np.diag([2/(w_k - 1.), 2/(h_k - 1.), 1]) # normalize\n",
    "    T_norm[:,2] = [-1, -1, 1] # translate\n",
    "    # Conditioning/Normalization of keypoints\n",
    "    kp_tf = kp_tf @ T_norm.T\n",
    "    return kp_tf, T_norm\n",
    "\n",
    "\n",
    "def NormalizeKeypoints(kp, w, h):\n",
    "    # Keypoints to homogeneous point coordinates\n",
    "    kp_tf = np.concatenate((kp, np.ones((np.shape(kp)[0],1))), axis=1)\n",
    "    # Define T_norm based on width and height of the image (now in camera coords)\n",
    "    T_norm = np.diag([2/(w - 1.), 2/(h - 1.), 1]) # normalize\n",
    "    T_norm[:,2] = [-1, -1, 1] # translate\n",
    "    # Conditioning/Normalization of keypoints\n",
    "    kp_tf = kp_tf @ T_norm.T\n",
    "    return kp_tf, T_norm\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "class FeatureExtractor:\n",
    "    def __init__(self):\n",
    "        self.extractor = cv2.SIFT_create()\n",
    "        \n",
    "    def compute_features(self, img):\n",
    "        pts = cv2.goodFeaturesToTrack(np.mean(img, axis=2).astype(np.uint8), 3000, qualityLevel=0.01, minDistance=7)\n",
    "        kps = [cv2.KeyPoint(x=f[0][0], y=f[0][1], size=20) for f in pts]\n",
    "        kp, des = self.extractor.compute(img, kps)\n",
    "        return kp, des\n",
    "        \n",
    "        #kp, des = self.extractor.detectAndCompute(img,None)\n",
    "        #return kp, des\n",
    "\n",
    "\n",
    "class FeatureMatcher:\n",
    "    def __init__(self):\n",
    "        self.matcher = cv2.BFMatcher()\n",
    "    def match_features(self, frame_prev, frame_cur):\n",
    "        kp1, desc1 = frame_prev.keypoints, frame_prev.features\n",
    "        kp2, desc2 = frame_cur.keypoints, frame_cur.features\n",
    "        # Match descriptors.\n",
    "        matches = self.matcher.knnMatch(desc1,desc2,k=1)\n",
    "        # Sort the matches according to nearest neighbor distance ratio (NNDR) (CV course, exercise 4)\n",
    "        distmat = np.dot(desc1, desc2.T)\n",
    "        X_terms = np.expand_dims(np.diag(np.dot(desc1, desc1.T)), axis=1)\n",
    "        X_terms = np.tile(X_terms,(1,desc2.shape[0]))\n",
    "        Y_terms = np.expand_dims(np.diag(np.dot(desc2, desc2.T)), axis=0)\n",
    "        Y_terms = np.tile(Y_terms,(desc1.shape[0],1))\n",
    "        distmat = np.sqrt(Y_terms + X_terms - 2*distmat)\n",
    "        ## We determine the mutually nearest neighbors\n",
    "        dist1 = np.amin(distmat, axis=1)\n",
    "        ids1 = np.argmin(distmat, axis=1)\n",
    "        dist2 = np.amin(distmat, axis=0)\n",
    "        ids2 = np.argmin(distmat, axis=0)\n",
    "        pairs = []\n",
    "        for k in range(ids1.size):\n",
    "            if k == ids2[ids1[k]]:\n",
    "                pairs.append(np.array([k, ids1[k], dist1[k]]))\n",
    "        pairs = np.array(pairs)\n",
    "        # We sort the mutually nearest neighbors based on the nearest neighbor distance ratio\n",
    "        NNDR = []\n",
    "        for k,ids1_k,dist1_k in pairs:\n",
    "            r_k = np.sort(distmat[int(k),:])\n",
    "            nndr = r_k[0]/r_k[1]\n",
    "            NNDR.append(nndr)\n",
    "\n",
    "        id_nnd = np.argsort(NNDR)\n",
    "        return np.array(matches)[id_nnd]\n",
    "\n",
    "class TransformationEstimator:\n",
    "    # Uses ransac algorithm to find best estimation of essential matrix between images\n",
    "    def __init__(self, N=10, Threshold=0.2):\n",
    "        self.N_iterations = N\n",
    "        self.Threshold = Threshold\n",
    "        \n",
    "    def F_from_point_pairs(xs,xss):\n",
    "        # xs, xss: Nx3 homologous point coordinates, N>7\n",
    "        # returns F: 3x3 Fundamental matrix\n",
    "        # Coefficient matrix\n",
    "        N = np.size(xs)[0]\n",
    "        A = np.zeros((N, 9))\n",
    "        for n in range(N):\n",
    "            A[n,:] = np.kron(xss[n,:], xs[n,:])\n",
    "        # Singular-value-decomposition\n",
    "        U,D,V = np.linalg.svd(A, full_matrices=True, compute_uv=True)\n",
    "        Fa = np.reshape(V[:,:,8], (3,3)) # approximation of F, could be > rank 2\n",
    "        Ua,Da,Va = np.linalg.svd(Fa, full_matrices=True, compute_uv=True)\n",
    "        F = (Ua * np.diag([Da[0,0], Da[1,1], 0])) @ Va\n",
    "        return F \n",
    "    \n",
    "    def E_from_point_pairs(xs,xss):\n",
    "        # xs, xss: Nx3 homologous point coordinates, N>7\n",
    "        # returns E: 3x3 Essential matrix\n",
    "        # Coefficient matrix\n",
    "        N = np.size(xs)[0]\n",
    "        A = np.zeros((N, 9))\n",
    "        for n in range(N):\n",
    "            A[n,:] = np.kron(xss[n,:], xs[n,:])\n",
    "        # Singular-value-decomposition\n",
    "        U,D,V = np.linalg.svd(A, full_matrices=True, compute_uv=True)\n",
    "        Ea = np.reshape(V[:,:,8], (3,3)) # approximation of F, could be > rank 2\n",
    "        Ua,Da,Va = np.linalg.svd(Ea, full_matrices=True, compute_uv=True)\n",
    "        E = (Ua * np.diag([1,1,0])) @ Va\n",
    "        return E\n",
    "      \n",
    "    def ransack(self, cur_frame_kp, prev_frame_kp, matches, T_norm, K):\n",
    "        # cur_frame_kp, prev_frame_kp: Nx2 pixel coordinates of keypoints matching between images\n",
    "        # T_norm, 3x3 transformation matrix of keypoints: center of mass to (0,0), x and y to scale [-1,1]\n",
    "        # K: camera calibration matrix\n",
    "        N = np.size(cur_frame_kp)[0]\n",
    "        highest_number_of_inliers = -1\n",
    "        best_essential = np.eye(3)\n",
    "        seed_n = 8 # has to be > 7\n",
    "        # Keypoints to homogeneous point coordinates\n",
    "        kp1 = np.concatenate((prev_frame_kp, np.ones(np.shape(N))))\n",
    "        kp2 = np.concatenate((cur_frame_kp, np.ones(np.shape(N))))\n",
    "        # Keypoints to camera coordinate system\n",
    "        kp1 = kp1 @ K.T\n",
    "        kp2 = kp2 @ K.T\n",
    "        # Conditioning/Normalization of keypoints\n",
    "        kp1 = kp1 @ T_norm.T\n",
    "        kp2 = kp2 @ T_norm.T\n",
    "        # Ransac loop\n",
    "        for n in range(self.N_iterations):\n",
    "            # Randomly select a seed group of > 7 matches\n",
    "            seed_group = np.random.randint(low=0, high=N, size=seed_n)\n",
    "            xs = kp1[seed_group,:]\n",
    "            xss = kp2[seed_group,:]\n",
    "            E = self.E_from_point_pairs(xs, xss)\n",
    "        \n",
    "        return(E)\n",
    "\n",
    "class Frame:\n",
    "    def __init__(self, rgb_fp, d_path, feature_extractor):\n",
    "        self.rgb = cv2.imread(rgb_fp)\n",
    "        # depth file read is handled bit differently\n",
    "        depth = cv2.imread(d_path)\n",
    "        self.depth =  Image.open(d_path)\n",
    "        self.keypoints, self.features  = None, None\n",
    "        self.feature_extractor = feature_extractor\n",
    "    def process_frame(self):\n",
    "        self.keypoints, self.features = self.feature_extract(self.rgb)\n",
    "        return self.keypoints, self.features, self.rgb\n",
    "        \n",
    "    def feature_extract(self, rgb):\n",
    "        return self.feature_extractor.compute_features(rgb)\n",
    "        \n",
    "def compute_fundamental_matrix(kp1, kp2, matches):\n",
    "    \"\"\"\n",
    "    Takes in filenames of two input images \n",
    "    Return Fundamental matrix computes \n",
    "    using 8 point algorithm\n",
    "    \"\"\"\n",
    "    \n",
    "    # extract points\n",
    "    pts1 = []\n",
    "    pts2 = []\n",
    "    for i,(m) in enumerate(matches):\n",
    "        #print(m.distance)\n",
    "        pts2.append(kp2[m[0].trainIdx].pt)\n",
    "        pts1.append(kp1[m[0].queryIdx].pt)\n",
    "    pts1  = np.asarray(pts1)\n",
    "    pts2 = np.asarray(pts2)\n",
    "    \n",
    "    # Compute fundamental matrix\n",
    "    F, mask = cv2.findFundamentalMat(pts1,pts2,cv2.FM_8POINT)\n",
    "    return F \n",
    "\n",
    "def essentialToRt(E):\n",
    "    # see wikipedia: https://en.wikipedia.org/wiki/Essential_matrix\n",
    "    U,d,Vt = np.linalg.svd(E)\n",
    "    W = np.mat([[0, -1, 0], [1, 0, 0], [0, 0, 1]])\n",
    "    Winv = W.T\n",
    "    # ansatz (educated guess)\n",
    "    #t = U @ W @ np.diag(d) @ U.T\n",
    "    t = Vt[-1,:]\n",
    "    R = U @ Winv @ Vt\n",
    "    \n",
    "    \n",
    "    return R, t\n",
    "\n",
    "debug = True\n",
    "scale = 5000\n",
    "D = np.array([0, 0, 0, 0], dtype=np.float32)  # no distortion\n",
    "K = np.matrix([[481.20, 0, 319.5], [0, 480.0, 239.5], [0, 0, 1]])  # camera intrinsic parameters\n",
    "fx, fy, cx, cy = 481.20, 480.0, 319.5, 239.5\n",
    "\n",
    "width, height = 640, 480\n",
    "\n",
    "def get_color(img, pt):\n",
    "        x = int(np.clip(pt[0], 0, width - 1))\n",
    "        y = int(np.clip(pt[1], 0, height - 1))\n",
    "        color = img[y, x]\n",
    "        if isinstance(color, Number):\n",
    "            color = np.array([color, color, color])\n",
    "        return color[::-1] / 255.\n",
    "\n",
    "def point2dTo3d(n, m, d):\n",
    "    z = float(d) / scale\n",
    "    x = (n - cx) * z / fx\n",
    "    y = (m - cy) * z / fy\n",
    "    point = np.array([x, y, z], dtype=np.float32)\n",
    "    return point\n",
    "\n",
    "\n",
    "def solvePnP(frame1, frame2, matches):\n",
    "        kp1, kp2, des1, des2, depth = frame1.keypoints, frame2.keypoints, frame1.features, frame2.features, frame1.depth\n",
    "        goodMatches = matches\n",
    "    \n",
    "        pts_obj, pts_img2, pts_img1 = [], [], []\n",
    "        colour = []\n",
    "        features = []\n",
    "        for i in range(0, len(goodMatches)):\n",
    "            p = kp1[goodMatches[i][0].queryIdx].pt\n",
    "            # d = depth[int(p[1])][int(p[0])]\n",
    "            d = depth.getpixel((int(p[0]), int(p[1])))\n",
    "            if d == 0:\n",
    "                pass\n",
    "            else:\n",
    "                p2 = kp2[goodMatches[i][0].trainIdx].pt\n",
    "                #dif = abs(cv2.norm(p) - cv2.norm(p2))\n",
    "                #if dif > .1:\n",
    "                    #print('dif -> {}'.format(dif))\n",
    "                    #pass\n",
    "                pts_img2.append(p2)\n",
    "                pts_img1.append(p)\n",
    "                pd = point2dTo3d(p[0], p[1], d)\n",
    "                pts_obj.append(pd)\n",
    "                # c = frame1.rgb[int(p[1])][int(p[0])]\n",
    "                colour.append(get_color(img=frame1.rgb, pt=p))\n",
    "                features.append(des1[goodMatches[i][0].queryIdx])\n",
    "\n",
    "        pts_obj, pts_img2 = np.array(pts_obj), np.array(pts_img2)\n",
    "        pts_img1 = np.array(pts_img1)\n",
    "        if debug:\n",
    "            print('pts_obj -> {}, pts_img->{}'.format(np.shape(pts_obj), np.shape(pts_img2)))\n",
    "            print(np.shape(cv2.solvePnPRansac(pts_obj, pts_img2, K, D, useExtrinsicGuess=False)))\n",
    "        \n",
    "        retval, rvec, tvec, inliers = cv2.solvePnPRansac(pts_obj, pts_img2, K, D, useExtrinsicGuess=False)\n",
    "        return retval, rvec, tvec, inliers\n",
    "\n",
    "def transformMatrix(rvec, tvec):\n",
    "        r, t = np.matrix(rvec), np.matrix(tvec)\n",
    "        R, _ = cv2.Rodrigues(r)\n",
    "        Rt = np.hstack((R, t))\n",
    "        T = np.vstack((Rt, np.matrix([0, 0, 0, 1])))\n",
    "        return T\n",
    "\n",
    "\n",
    "def opencv_R_t(E,kp1, kp2, matches):\n",
    "    \"\"\"\n",
    "    Takes in filenames of two input images \n",
    "    Return Fundamental matrix computes \n",
    "    using 8 point algorithm\n",
    "    \"\"\"\n",
    "    \n",
    "    # extract points\n",
    "    pts1 = []\n",
    "    pts2 = []\n",
    "    for i,(m) in enumerate(matches):\n",
    "        #print(m.distance)\n",
    "        pts2.append(kp2[m[0].trainIdx].pt)\n",
    "        pts1.append(kp1[m[0].queryIdx].pt)\n",
    "    pts1  = np.asarray(pts1)\n",
    "    pts2 = np.asarray(pts2)\n",
    "    \n",
    "    pts_l_norm = cv2.undistortPoints(np.expand_dims(pts1, axis=1), cameraMatrix=K, distCoeffs=None)\n",
    "    pts_r_norm = cv2.undistortPoints(np.expand_dims(pts2, axis=1), cameraMatrix=K, distCoeffs=None)\n",
    "\n",
    "    points, R, t, mask = cv2.recoverPose(E,pts_l_norm, pts_r_norm)\n",
    "    return R,t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9.99779581e-01 -1.25375812e-02 -1.68403866e-02]\n",
      " [ 1.25463071e-02  9.99921207e-01  4.12599352e-04]\n",
      " [ 1.68338867e-02 -6.23793068e-04  9.99858106e-01]]\n",
      "[[-0.87830467 -0.01602732 -0.47783263]]\n",
      "pts_obj -> (275, 3), pts_img->(275, 2)\n",
      "(4,)\n",
      "[[ 0.99972268 -0.0138756  -0.01902727 -0.01565997]\n",
      " [ 0.01377587  0.99989073 -0.00536223  0.01409231]\n",
      " [ 0.01909959  0.00509863  0.99980459  0.01628497]\n",
      " [ 0.          0.          0.          1.        ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juuso/visual_slam/venv/lib/python3.8/site-packages/numpy/core/fromnumeric.py:2009: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  result = asarray(a).shape\n"
     ]
    }
   ],
   "source": [
    "# Filepaths\n",
    "cur_dir = \"/home/juuso\"\n",
    "dir_rgb = cur_dir + \"/visual_slam/data/ICL_NUIM/rgb/\"\n",
    "dir_depth = cur_dir + \"/visual_slam/data/ICL_NUIM/depth/\"\n",
    "is_WINDOWS = False\n",
    "if is_WINDOWS:\n",
    "    dir_rgb = dir_rgb.replace(\"/\", \"\\\\\")\n",
    "    dir_depth = dir_depth.replace(\"/\", \"\\\\\")\n",
    "# Initialize\n",
    "feature_extractor = FeatureExtractor()\n",
    "feature_matcher = FeatureMatcher()\n",
    "\n",
    "\n",
    "# run feature extraction for 1st image\n",
    "fp_rgb = dir_rgb + str(1) + \".png\"\n",
    "fp_depth = dir_depth + str(1) + \".png\"\n",
    "cur_frame = Frame(fp_rgb, fp_depth, feature_extractor)\n",
    "kp1, features1, rgb1 = cur_frame.process_frame() \n",
    "\n",
    "prev_frame = cur_frame\n",
    "i = 30 # jump to 30th frame\n",
    "\n",
    "fp_rgb = dir_rgb + str(i) + \".png\"\n",
    "fp_depth = dir_depth + str(i) + \".png\"\n",
    "# Feature Extraction for current frame\n",
    "cur_frame = Frame(fp_rgb, fp_depth, feature_extractor)\n",
    "kp2, features2, rgb2 = cur_frame.process_frame()\n",
    "\n",
    "# Feature Matching to previous frame\n",
    "matches = feature_matcher.match_features(prev_frame, cur_frame) \n",
    "\n",
    "best8 = matches[:8]\n",
    "\n",
    "F = compute_fundamental_matrix(kp1, kp2, matches[:100])\n",
    "F\n",
    "#img3 = cv2.drawMatchesKnn(prev_frame.rgb,prev_frame.keypoints, cur_frame.rgb,cur_frame.keypoints,matches[:8],None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "#img2 = cv2.drawKeypoints(rgb, kp, None, color=(0,255,0), flags=0)\n",
    "#cv2.imshow('a', img3)\n",
    "#cv2.waitKey(0)\n",
    "E = K.T @ F @ K\n",
    "#print(E)\n",
    "\n",
    "R, t = essentialToRt(E)\n",
    "print(-R)\n",
    "print(t)\n",
    "\n",
    "#t = np.array([t_mat[2,1], t_mat[0,2], t_mat[1,0]])\n",
    "#t = t/np.linalg.norm(t)\n",
    "#print(np.linalg.norm(t))\n",
    "# E = U@np.diag(S)@VT\n",
    "print(\"prööt\")\n",
    "\n",
    "retval, rvec, tvec, inliers = solvePnP(prev_frame, cur_frame, matches)\n",
    "\n",
    "\n",
    "T = transformMatrix(rvec, tvec)\n",
    "\n",
    "print(T)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.6.0) :-1: error: (-5:Bad argument) in function 'findFundamentalMat'\n> Overload resolution failed:\n>  - findFundamentalMat() missing required argument 'ransacReprojThreshold' (pos 4)\n>  - findFundamentalMat() missing required argument 'ransacReprojThreshold' (pos 4)\n>  - points1 data type = 17 is not supported\n>  - Expected Ptr<cv::UMat> for argument 'points1'\n>  - points1 data type = 17 is not supported\n>  - Expected Ptr<cv::UMat> for argument 'points1'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [17], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m pts1  \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(pts1)\n\u001b[1;32m     12\u001b[0m pts2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(pts2)\n\u001b[0;32m---> 15\u001b[0m F, mask \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfindFundamentalMat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpts1\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpts2\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFM_8POINT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx2.T @ F @ x1 = (should be close to 0)\u001b[39m\u001b[38;5;124m\"\u001b[39m )\n\u001b[1;32m     19\u001b[0m test_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.6.0) :-1: error: (-5:Bad argument) in function 'findFundamentalMat'\n> Overload resolution failed:\n>  - findFundamentalMat() missing required argument 'ransacReprojThreshold' (pos 4)\n>  - findFundamentalMat() missing required argument 'ransacReprojThreshold' (pos 4)\n>  - points1 data type = 17 is not supported\n>  - Expected Ptr<cv::UMat> for argument 'points1'\n>  - points1 data type = 17 is not supported\n>  - Expected Ptr<cv::UMat> for argument 'points1'\n"
     ]
    }
   ],
   "source": [
    "#print(np.array(kp1))\n",
    "\n",
    "# extract points\n",
    "pts1 = []\n",
    "pts2 = []\n",
    "for i,(m) in enumerate(matches):\n",
    "    #print(m.distance)\n",
    "    pts2.append(kp2[m[0].trainIdx])\n",
    "    pts1.append(kp1[m[0].queryIdx])\n",
    "\n",
    "pts1  = np.asarray(pts1)\n",
    "pts2 = np.asarray(pts2)\n",
    "\n",
    "\n",
    "F, mask = cv2.findFundamentalMat(pts1[:100], pts2[:100], cv2.FM_8POINT)\n",
    "print(\"x2.T @ F @ x1 = (should be close to 0)\" )\n",
    "\n",
    "\n",
    "test_idx = 20\n",
    "\n",
    "x1 = np.expand_dims(pts1[test_idx,:], axis=1)\n",
    "x2 = np.expand_dims(pts2[test_idx,:], axis=1)\n",
    "\n",
    "\n",
    "#print(np.shape(x2))\n",
    "\n",
    "\n",
    "\n",
    "print(x2.T @ F @ x1)\n",
    "\n",
    "E = K.T @ F @ K # Get the essential matrix\n",
    "\n",
    "[U,S,V] = np.linalg.svd(E)\n",
    "\n",
    "diag_110 = np.array([[1, 0, 0],[0, 1, 0],[0, 0, 0]])\n",
    "newE = U@diag_110@V\n",
    "[U,S,V] = np.linalg.svd(newE); # Perform second decompose to get S=diag(1,1,0)\n",
    "\n",
    "W = np.array([[0, -1, 0],[1, 0, 0],[0, 0, 1]]) # [0 -1 0; 1 0 0; 0 0 1];\n",
    "\n",
    "R1 = U@W@V\n",
    "R2 = U@W.T@V\n",
    "t1 = U[:,2] #norm = 1\n",
    "t2 = -U[:,2] #norm = 1\n",
    "\n",
    "\n",
    "#print(R1, t1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T_norm = np.diag([2/(1920 - 1.), 2/(1080 - 1.), 1]) # normalize\n",
    "T_norm[:,2] = [-1, -1, 1] # translate\n",
    "\n",
    "a = np.array([1920,1080,1]) @ np.linalg.inv(K.T)\n",
    "\n",
    "np.shape(np.squeeze(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[211, 149, 231, 226, 176, 131, 171, 133]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[297, 227, 276, 206, 201, 275, 152, 101]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print([m[0].queryIdx for m in best8])\n",
    "[m[0].trainIdx for m in best8]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4e3256bb47a9a0005e38ffa68a9c5479a1950b0272581dfa4d7d4e2ef8ce54e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
