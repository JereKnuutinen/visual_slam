{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "/usr/lib/python3/dist-packages/g2o.cpython-38-x86_64-linux-gnu.so: undefined symbol: cs_di_schol",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mg2o\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: /usr/lib/python3/dist-packages/g2o.cpython-38-x86_64-linux-gnu.so: undefined symbol: cs_di_schol"
     ]
    }
   ],
   "source": [
    "import g2o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "/home/juuso/visual_slam/venv/lib/python3.8/site-packages/g2o.cpython-38-x86_64-linux-gnu.so: undefined symbol: cs_di_schol",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mre\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mPIL\u001b[39;00m \u001b[39mimport\u001b[39;00m Image\n\u001b[0;32m----> 9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mg2o\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mKeyPoints2CameraCoords\u001b[39m(kp, K, w, h):\n\u001b[1;32m     12\u001b[0m     \u001b[39m# Keypoints to homogeneous point coordinates\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     kp_tf \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate((kp, np\u001b[39m.\u001b[39mones((np\u001b[39m.\u001b[39mshape(kp)[\u001b[39m0\u001b[39m],\u001b[39m1\u001b[39m))), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: /home/juuso/visual_slam/venv/lib/python3.8/site-packages/g2o.cpython-38-x86_64-linux-gnu.so: undefined symbol: cs_di_schol"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "import os \n",
    "from pathlib import Path\n",
    "from numbers import Number\n",
    "import re\n",
    "from PIL import Image\n",
    "import g2o\n",
    "\n",
    "def KeyPoints2CameraCoords(kp, K, w, h):\n",
    "    # Keypoints to homogeneous point coordinates\n",
    "    kp_tf = np.concatenate((kp, np.ones((np.shape(kp)[0],1))), axis=1)\n",
    "    # Keypoints to camera coordinate system\n",
    "    kp_tf = kp_tf @ np.linalg.inv(K.T)\n",
    "    limits = np.squeeze(np.array([w, h, 1]) @ np.linalg.inv(K.T)) # e.g. [1920,1080,1] to camera coords\n",
    "    w_k, h_k = limits[0,0], limits[0,1]\n",
    "    # Define T_norm based on width and height of the image (now in camera coords)\n",
    "    T_norm = np.diag([2/(w_k - 1.), 2/(h_k - 1.), 1]) # normalize\n",
    "    T_norm[:,2] = [-1, -1, 1] # translate\n",
    "    # Conditioning/Normalization of keypoints\n",
    "    kp_tf = kp_tf @ T_norm.T\n",
    "    return kp_tf, T_norm\n",
    "\n",
    "\n",
    "def NormalizeKeypoints(kp, w, h):\n",
    "    # Keypoints to homogeneous point coordinates\n",
    "    kp_tf = np.concatenate((kp, np.ones((np.shape(kp)[0],1))), axis=1)\n",
    "    # Define T_norm based on width and height of the image (now in camera coords)\n",
    "    T_norm = np.diag([2/(w - 1.), 2/(h - 1.), 1]) # normalize\n",
    "    T_norm[:,2] = [-1, -1, 1] # translate\n",
    "    # Conditioning/Normalization of keypoints\n",
    "    kp_tf = kp_tf @ T_norm.T\n",
    "    return kp_tf, T_norm\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "class FeatureExtractor:\n",
    "    def __init__(self):\n",
    "        self.extractor = cv2.SIFT_create()\n",
    "        \n",
    "    def compute_features(self, img):\n",
    "        pts = cv2.goodFeaturesToTrack(np.mean(img, axis=2).astype(np.uint8), 3000, qualityLevel=0.01, minDistance=7)\n",
    "        kps = [cv2.KeyPoint(x=f[0][0], y=f[0][1], size=20) for f in pts]\n",
    "        kp, des = self.extractor.compute(img, kps)\n",
    "        return kp, des\n",
    "        \n",
    "        #kp, des = self.extractor.detectAndCompute(img,None)\n",
    "        #return kp, des\n",
    "\n",
    "\n",
    "class FeatureMatcher:\n",
    "    def __init__(self):\n",
    "        self.matcher = cv2.BFMatcher()\n",
    "    def match_features(self, frame_prev, frame_cur):\n",
    "        kp1, desc1 = frame_prev.keypoints, frame_prev.features\n",
    "        kp2, desc2 = frame_cur.keypoints, frame_cur.features\n",
    "        # Match descriptors.\n",
    "        matches = self.matcher.knnMatch(desc1,desc2,k=1)\n",
    "        # Sort the matches according to nearest neighbor distance ratio (NNDR) (CV course, exercise 4)\n",
    "        distmat = np.dot(desc1, desc2.T)\n",
    "        X_terms = np.expand_dims(np.diag(np.dot(desc1, desc1.T)), axis=1)\n",
    "        X_terms = np.tile(X_terms,(1,desc2.shape[0]))\n",
    "        Y_terms = np.expand_dims(np.diag(np.dot(desc2, desc2.T)), axis=0)\n",
    "        Y_terms = np.tile(Y_terms,(desc1.shape[0],1))\n",
    "        distmat = np.sqrt(Y_terms + X_terms - 2*distmat)\n",
    "        ## We determine the mutually nearest neighbors\n",
    "        dist1 = np.amin(distmat, axis=1)\n",
    "        ids1 = np.argmin(distmat, axis=1)\n",
    "        dist2 = np.amin(distmat, axis=0)\n",
    "        ids2 = np.argmin(distmat, axis=0)\n",
    "        pairs = []\n",
    "        for k in range(ids1.size):\n",
    "            if k == ids2[ids1[k]]:\n",
    "                pairs.append(np.array([k, ids1[k], dist1[k]]))\n",
    "        pairs = np.array(pairs)\n",
    "        # We sort the mutually nearest neighbors based on the nearest neighbor distance ratio\n",
    "        NNDR = []\n",
    "        for k,ids1_k,dist1_k in pairs:\n",
    "            r_k = np.sort(distmat[int(k),:])\n",
    "            nndr = r_k[0]/r_k[1]\n",
    "            NNDR.append(nndr)\n",
    "\n",
    "        id_nnd = np.argsort(NNDR)\n",
    "        return np.array(matches)[id_nnd]\n",
    "\n",
    "class TransformationEstimator:\n",
    "    # Uses ransac algorithm to find best estimation of essential matrix between images\n",
    "    def __init__(self, N=10, Threshold=0.2):\n",
    "        self.N_iterations = N\n",
    "        self.Threshold = Threshold\n",
    "        \n",
    "    def F_from_point_pairs(xs,xss):\n",
    "        # xs, xss: Nx3 homologous point coordinates, N>7\n",
    "        # returns F: 3x3 Fundamental matrix\n",
    "        # Coefficient matrix\n",
    "        N = np.size(xs)[0]\n",
    "        A = np.zeros((N, 9))\n",
    "        for n in range(N):\n",
    "            A[n,:] = np.kron(xss[n,:], xs[n,:])\n",
    "        # Singular-value-decomposition\n",
    "        U,D,V = np.linalg.svd(A, full_matrices=True, compute_uv=True)\n",
    "        Fa = np.reshape(V[:,:,8], (3,3)) # approximation of F, could be > rank 2\n",
    "        Ua,Da,Va = np.linalg.svd(Fa, full_matrices=True, compute_uv=True)\n",
    "        F = (Ua * np.diag([Da[0,0], Da[1,1], 0])) @ Va\n",
    "        return F \n",
    "    \n",
    "    def E_from_point_pairs(xs,xss):\n",
    "        # xs, xss: Nx3 homologous point coordinates, N>7\n",
    "        # returns E: 3x3 Essential matrix\n",
    "        # Coefficient matrix\n",
    "        N = np.size(xs)[0]\n",
    "        A = np.zeros((N, 9))\n",
    "        for n in range(N):\n",
    "            A[n,:] = np.kron(xss[n,:], xs[n,:])\n",
    "        # Singular-value-decomposition\n",
    "        U,D,V = np.linalg.svd(A, full_matrices=True, compute_uv=True)\n",
    "        Ea = np.reshape(V[:,:,8], (3,3)) # approximation of F, could be > rank 2\n",
    "        Ua,Da,Va = np.linalg.svd(Ea, full_matrices=True, compute_uv=True)\n",
    "        E = (Ua * np.diag([1,1,0])) @ Va\n",
    "        return E\n",
    "      \n",
    "    def ransack(self, cur_frame_kp, prev_frame_kp, matches, T_norm, K):\n",
    "        # cur_frame_kp, prev_frame_kp: Nx2 pixel coordinates of keypoints matching between images\n",
    "        # T_norm, 3x3 transformation matrix of keypoints: center of mass to (0,0), x and y to scale [-1,1]\n",
    "        # K: camera calibration matrix\n",
    "        N = np.size(cur_frame_kp)[0]\n",
    "        highest_number_of_inliers = -1\n",
    "        best_essential = np.eye(3)\n",
    "        seed_n = 8 # has to be > 7\n",
    "        # Keypoints to homogeneous point coordinates\n",
    "        kp1 = np.concatenate((prev_frame_kp, np.ones(np.shape(N))))\n",
    "        kp2 = np.concatenate((cur_frame_kp, np.ones(np.shape(N))))\n",
    "        # Keypoints to camera coordinate system\n",
    "        kp1 = kp1 @ K.T\n",
    "        kp2 = kp2 @ K.T\n",
    "        # Conditioning/Normalization of keypoints\n",
    "        kp1 = kp1 @ T_norm.T\n",
    "        kp2 = kp2 @ T_norm.T\n",
    "        # Ransac loop\n",
    "        for n in range(self.N_iterations):\n",
    "            # Randomly select a seed group of > 7 matches\n",
    "            seed_group = np.random.randint(low=0, high=N, size=seed_n)\n",
    "            xs = kp1[seed_group,:]\n",
    "            xss = kp2[seed_group,:]\n",
    "            E = self.E_from_point_pairs(xs, xss)\n",
    "        \n",
    "        return(E)\n",
    "\n",
    "class Frame:\n",
    "    def __init__(self, rgb_fp, d_path, feature_extractor):\n",
    "        self.rgb = cv2.imread(rgb_fp)\n",
    "        # depth file read is handled bit differently\n",
    "        depth = cv2.imread(d_path)\n",
    "        self.depth =  Image.open(d_path)\n",
    "        self.keypoints, self.features  = None, None\n",
    "        self.feature_extractor = feature_extractor\n",
    "    def process_frame(self):\n",
    "        self.keypoints, self.features = self.feature_extract(self.rgb)\n",
    "        return self.keypoints, self.features, self.rgb\n",
    "        \n",
    "    def feature_extract(self, rgb):\n",
    "        return self.feature_extractor.compute_features(rgb)\n",
    "        \n",
    "def compute_fundamental_matrix(kp1, kp2, matches):\n",
    "    \"\"\"\n",
    "    Takes in filenames of two input images \n",
    "    Return Fundamental matrix computes \n",
    "    using 8 point algorithm\n",
    "    \"\"\"\n",
    "    \n",
    "    # extract points\n",
    "    pts1 = []\n",
    "    pts2 = []\n",
    "    for i,(m) in enumerate(matches):\n",
    "        #print(m.distance)\n",
    "        pts2.append(kp2[m[0].trainIdx].pt)\n",
    "        pts1.append(kp1[m[0].queryIdx].pt)\n",
    "    pts1  = np.asarray(pts1)\n",
    "    pts2 = np.asarray(pts2)\n",
    "    \n",
    "    # Compute fundamental matrix\n",
    "    F, mask = cv2.findFundamentalMat(pts1,pts2,cv2.FM_8POINT)\n",
    "    return F \n",
    "\n",
    "def essentialToRt(E, opt = 1):\n",
    "    # see wikipedia: https://en.wikipedia.org/wiki/Essential_matrix\n",
    "    U,d,Vt = np.linalg.svd(E)\n",
    "    W = np.mat([[0, -1, 0], [1, 0, 0], [0, 0, 1]])\n",
    "    Winv = W.T\n",
    "    # ansatz (educated guess)\n",
    "    #t = U @ W @ np.diag(d) @ U.T\n",
    "    #t = Vt[-1,:]\n",
    "    R = U @ Winv @ Vt\n",
    "    Z = np.mat([[0, 1, 0], [-1, 0, 0], [0, 0, 0]]) \n",
    "    if opt==1:\n",
    "        t = U @ Z @ U.T\n",
    "        t = np.array([t[2,1], t[0,2], t[1,0]]) # get vector from transpose operation\n",
    "    elif opt==2:\n",
    "        t = U @ W @ np.diag(d) @ U.T\n",
    "        t = np.array([t[2,1], t[0,2], t[1,0]]) \n",
    "    else:\n",
    "        t = Vt[-1,:]\n",
    "    return R, t\n",
    "\n",
    "debug = True\n",
    "scale = 5000\n",
    "D = np.array([0, 0, 0, 0], dtype=np.float32)  # no distortion\n",
    "K = np.matrix([[481.20, 0, 319.5], [0, 480.0, 239.5], [0, 0, 1]])  # camera intrinsic parameters\n",
    "fx, fy, cx, cy = 481.20, 480.0, 319.5, 239.5\n",
    "\n",
    "width, height = 640, 480\n",
    "\n",
    "def get_color(img, pt):\n",
    "        x = int(np.clip(pt[0], 0, width - 1))\n",
    "        y = int(np.clip(pt[1], 0, height - 1))\n",
    "        color = img[y, x]\n",
    "        if isinstance(color, Number):\n",
    "            color = np.array([color, color, color])\n",
    "        return color[::-1] / 255.\n",
    "\n",
    "def point2dTo3d(n, m, d):\n",
    "    z = float(d) / scale\n",
    "    x = (n - cx) * z / fx\n",
    "    y = (m - cy) * z / fy\n",
    "    point = np.array([x, y, z], dtype=np.float32)\n",
    "    return point\n",
    "\n",
    "\n",
    "def solvePnP(frame1, frame2, matches):\n",
    "        kp1, kp2, des1, des2, depth = frame1.keypoints, frame2.keypoints, frame1.features, frame2.features, frame1.depth\n",
    "        goodMatches = matches\n",
    "    \n",
    "        pts_obj, pts_img2, pts_img1 = [], [], []\n",
    "        colour = []\n",
    "        features = []\n",
    "        for i in range(0, len(goodMatches)):\n",
    "            p = kp1[goodMatches[i][0].queryIdx].pt\n",
    "            # d = depth[int(p[1])][int(p[0])]\n",
    "            d = depth.getpixel((int(p[0]), int(p[1])))\n",
    "            if d == 0:\n",
    "                pass\n",
    "            else:\n",
    "                p2 = kp2[goodMatches[i][0].trainIdx].pt\n",
    "                #dif = abs(cv2.norm(p) - cv2.norm(p2))\n",
    "                #if dif > .1:\n",
    "                    #print('dif -> {}'.format(dif))\n",
    "                    #pass\n",
    "                pts_img2.append(p2)\n",
    "                pts_img1.append(p)\n",
    "                pd = point2dTo3d(p[0], p[1], d)\n",
    "                pts_obj.append(pd)\n",
    "                # c = frame1.rgb[int(p[1])][int(p[0])]\n",
    "                colour.append(get_color(img=frame1.rgb, pt=p))\n",
    "                features.append(des1[goodMatches[i][0].queryIdx])\n",
    "\n",
    "        pts_obj, pts_img2 = np.array(pts_obj), np.array(pts_img2)\n",
    "        pts_img1 = np.array(pts_img1)\n",
    "        if debug:\n",
    "            print('pts_obj -> {}, pts_img->{}'.format(np.shape(pts_obj), np.shape(pts_img2)))\n",
    "            print(np.shape(cv2.solvePnPRansac(pts_obj, pts_img2, K, D, useExtrinsicGuess=False)))\n",
    "        \n",
    "        retval, rvec, tvec, inliers = cv2.solvePnPRansac(pts_obj, pts_img2, K, D, useExtrinsicGuess=False)\n",
    "        return retval, rvec, tvec, inliers\n",
    "\n",
    "def transformMatrix(rvec, tvec):\n",
    "        r, t = np.matrix(rvec), np.matrix(tvec)\n",
    "        R, _ = cv2.Rodrigues(r)\n",
    "        Rt = np.hstack((R, t))\n",
    "        T = np.vstack((Rt, np.matrix([0, 0, 0, 1])))\n",
    "        return T\n",
    "\n",
    "\n",
    "def opencv_R_t(E,kp1, kp2, matches):\n",
    "    \"\"\" \n",
    "    Return Fundamental matrix computes \n",
    "    using 8 point algorithm\n",
    "    \"\"\"\n",
    "    \n",
    "    # extract points\n",
    "    pts1 = []\n",
    "    pts2 = []\n",
    "    for i,(m) in enumerate(matches):\n",
    "        #print(m.distance)\n",
    "        pts2.append(kp2[m[0].trainIdx].pt)\n",
    "        pts1.append(kp1[m[0].queryIdx].pt)\n",
    "    pts1  = np.asarray(pts1)\n",
    "    pts2 = np.asarray(pts2)\n",
    "    \n",
    "    pts_l_norm = cv2.undistortPoints(np.expand_dims(pts1, axis=1), cameraMatrix=K, distCoeffs=None)\n",
    "    pts_r_norm = cv2.undistortPoints(np.expand_dims(pts2, axis=1), cameraMatrix=K, distCoeffs=None)\n",
    "\n",
    "    points, R, t, mask = cv2.recoverPose(E,pts_l_norm, pts_r_norm)\n",
    "    return R,t\n",
    "\n",
    "\n",
    "def triangulation(kp1, kp2, T_1w, T_2w):\n",
    "    \"\"\"Triangulation to get 3D points\n",
    "    Args:\n",
    "        kp1 (Nx2): keypoint in view 1 (normalized)\n",
    "        kp2 (Nx2): keypoints in view 2 (normalized)\n",
    "        T_1w (4x4): pose of view 1 w.r.t  i.e. T_1w (from w to 1)\n",
    "        T_2w (4x4): pose of view 2 w.r.t world, i.e. T_2w (from w to 2)\n",
    "    Returns:\n",
    "        X (3xN): 3D coordinates of the keypoints w.r.t world coordinate\n",
    "        X1 (3xN): 3D coordinates of the keypoints w.r.t view1 coordinate\n",
    "        X2 (3xN): 3D coordinates of the keypoints w.r.t view2 coordinate\n",
    "    \"\"\"\n",
    "    kp1_3D = np.ones((3, kp1.shape[0]))\n",
    "    kp2_3D = np.ones((3, kp2.shape[0]))\n",
    "    kp1_3D[0], kp1_3D[1] = kp1[:, 0].copy(), kp1[:, 1].copy()\n",
    "    kp2_3D[0], kp2_3D[1] = kp2[:, 0].copy(), kp2[:, 1].copy()\n",
    "    X = cv2.triangulatePoints(T_1w[:3], T_2w[:3], kp1_3D[:2], kp2_3D[:2])\n",
    "    X /= X[3]\n",
    "    X1 = T_1w[:3] @ X\n",
    "    X2 = T_2w[:3] @ X\n",
    "    return X[:3], X1, X2 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FeatureExtractor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m     dir_depth \u001b[38;5;241m=\u001b[39m dir_depth\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Initialize\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m feature_extractor \u001b[38;5;241m=\u001b[39m \u001b[43mFeatureExtractor\u001b[49m()\n\u001b[1;32m     11\u001b[0m feature_matcher \u001b[38;5;241m=\u001b[39m FeatureMatcher()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# run feature extraction for 1st image\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'FeatureExtractor' is not defined"
     ]
    }
   ],
   "source": [
    "# Filepaths\n",
    "cur_dir = \"/home/juuso\"\n",
    "dir_rgb = cur_dir + \"/visual_slam/data/ICL_NUIM/rgb/\"\n",
    "dir_depth = cur_dir + \"/visual_slam/data/ICL_NUIM/depth/\"\n",
    "is_WINDOWS = False\n",
    "if is_WINDOWS:\n",
    "    dir_rgb = dir_rgb.replace(\"/\", \"\\\\\")\n",
    "    dir_depth = dir_depth.replace(\"/\", \"\\\\\")\n",
    "# Initialize\n",
    "feature_extractor = FeatureExtractor()\n",
    "feature_matcher = FeatureMatcher()\n",
    "# run feature extraction for 1st image\n",
    "fp_rgb = dir_rgb + str(1) + \".png\"\n",
    "fp_depth = dir_depth + str(1) + \".png\"\n",
    "cur_frame = Frame(fp_rgb, fp_depth, feature_extractor)\n",
    "kp, features, rgb = cur_frame.process_frame() \n",
    "prev_frame = cur_frame\n",
    "\n",
    "for i in range(2,1000):\n",
    "    if i % 30 == 0:\n",
    "        fp_rgb = dir_rgb + str(i) + \".png\"\n",
    "        fp_depth = dir_depth + str(i) + \".png\"\n",
    "        # Feature Extraction for current frame\n",
    "        cur_frame = Frame(fp_rgb, fp_depth, feature_extractor)\n",
    "        kp, features, rgb = cur_frame.process_frame()\n",
    "        # Feature Matching to previous frame\n",
    "        matches = feature_matcher.match_features(prev_frame, cur_frame)    \n",
    "        # TODO: if not enough matches (<100) continue to next frame\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # TODO: compute homography and inliers\n",
    "        \n",
    "        # TODO: compute essential and inliers\n",
    "        \n",
    "        # TODO: choose between models based on number of inliers\n",
    "        \n",
    "        # TODO: if number of inliers too low continue to next frame\n",
    "        \n",
    "        # TODO: get pose transformation\n",
    "        \n",
    "        # TODO: triangulate two view to obtain 3-D map points\n",
    "        \n",
    "        \n",
    "        # Display\n",
    "        img3 = cv2.drawMatchesKnn(prev_frame.rgb,prev_frame.keypoints, cur_frame.rgb,cur_frame.keypoints,matches[:100],None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "        #img2 = cv2.drawKeypoints(rgb, kp, None, color=(0,255,0), flags=0)\n",
    "        cv2.imshow('a', img3)\n",
    "        cv2.waitKey(0)\n",
    "        #\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        prev_frame = cur_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handcrafted\n",
      "[[-9.99779581e-01  1.25375812e-02  1.68403866e-02]\n",
      " [-1.25463071e-02 -9.99921207e-01 -4.12599352e-04]\n",
      " [-1.68338867e-02  6.23793068e-04 -9.99858106e-01]]\n",
      "[-0.86986325 -0.02724269 -0.49254011]\n",
      "1.0000000000000004\n",
      "opencv\n",
      "[[ 9.99779581e-01 -1.25375812e-02 -1.68403866e-02]\n",
      " [ 1.25463071e-02  9.99921207e-01  4.12599352e-04]\n",
      " [ 1.68338867e-02 -6.23793068e-04  9.99858106e-01]]\n",
      "[[-0.86986325]\n",
      " [-0.02724269]\n",
      " [-0.49254011]]\n",
      "Ground truth\n",
      "pts_obj -> (275, 3), pts_img->(275, 2)\n",
      "(4,)\n",
      "[[ 0.99972268 -0.0138756  -0.01902727 -0.01565997]\n",
      " [ 0.01377587  0.99989073 -0.00536223  0.01409231]\n",
      " [ 0.01909959  0.00509863  0.99980459  0.01628497]\n",
      " [ 0.          0.          0.          1.        ]]\n",
      "0.026627577148395667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juuso/visual_slam/venv/lib/python3.8/site-packages/numpy/core/fromnumeric.py:2009: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  result = asarray(a).shape\n"
     ]
    }
   ],
   "source": [
    "# Filepaths\n",
    "cur_dir = \"/home/juuso\"\n",
    "dir_rgb = cur_dir + \"/visual_slam/data/ICL_NUIM/rgb/\"\n",
    "dir_depth = cur_dir + \"/visual_slam/data/ICL_NUIM/depth/\"\n",
    "is_WINDOWS = False\n",
    "if is_WINDOWS:\n",
    "    dir_rgb = dir_rgb.replace(\"/\", \"\\\\\")\n",
    "    dir_depth = dir_depth.replace(\"/\", \"\\\\\")\n",
    "# Initialize\n",
    "feature_extractor = FeatureExtractor()\n",
    "feature_matcher = FeatureMatcher()\n",
    "\n",
    "\n",
    "# run feature extraction for 1st image\n",
    "fp_rgb = dir_rgb + str(1) + \".png\"\n",
    "fp_depth = dir_depth + str(1) + \".png\"\n",
    "cur_frame = Frame(fp_rgb, fp_depth, feature_extractor)\n",
    "kp1, features1, rgb1 = cur_frame.process_frame() \n",
    "\n",
    "prev_frame = cur_frame\n",
    "i = 30 # jump to 30th frame\n",
    "\n",
    "fp_rgb = dir_rgb + str(i) + \".png\"\n",
    "fp_depth = dir_depth + str(i) + \".png\"\n",
    "# Feature Extraction for current frame\n",
    "cur_frame = Frame(fp_rgb, fp_depth, feature_extractor)\n",
    "kp2, features2, rgb2 = cur_frame.process_frame()\n",
    "\n",
    "# Feature Matching to previous frame\n",
    "matches = feature_matcher.match_features(prev_frame, cur_frame) \n",
    "\n",
    "best8 = matches[:8]\n",
    "\n",
    "F = compute_fundamental_matrix(kp1, kp2, matches[:100])\n",
    "F\n",
    "#img3 = cv2.drawMatchesKnn(prev_frame.rgb,prev_frame.keypoints, cur_frame.rgb,cur_frame.keypoints,matches[:8],None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "#img2 = cv2.drawKeypoints(rgb, kp, None, color=(0,255,0), flags=0)\n",
    "#cv2.imshow('a', img3)\n",
    "#cv2.waitKey(0)\n",
    "E = K.T @ F @ K\n",
    "#print(E)\n",
    "\n",
    "print(\"Handcrafted\")\n",
    "R, t = essentialToRt(E)\n",
    "t = np.array([t[2,1], t[0,2], t[1,0]])\n",
    "print(R)\n",
    "print(t)\n",
    "\n",
    "print(np.linalg.norm(t))\n",
    "\n",
    "\n",
    "print(\"opencv\")\n",
    "\n",
    "R, t = opencv_R_t(E, kp1, kp2, matches[:100])\n",
    "print(R)\n",
    "print(t)\n",
    "\n",
    "\n",
    "#t = np.array([t_mat[2,1], t_mat[0,2], t_mat[1,0]])\n",
    "#t = t/np.linalg.norm(t)\n",
    "#print(np.linalg.norm(t))\n",
    "# E = U@np.diag(S)@VT\n",
    "\n",
    "\n",
    "print(\"Ground truth\")\n",
    "retval, rvec, tvec, inliers = solvePnP(prev_frame, cur_frame, matches)\n",
    "T = transformMatrix(rvec, tvec)\n",
    "print(T)\n",
    "print(np.linalg.norm(tvec))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.6.0) :-1: error: (-5:Bad argument) in function 'findFundamentalMat'\n> Overload resolution failed:\n>  - findFundamentalMat() missing required argument 'ransacReprojThreshold' (pos 4)\n>  - findFundamentalMat() missing required argument 'ransacReprojThreshold' (pos 4)\n>  - points1 data type = 17 is not supported\n>  - Expected Ptr<cv::UMat> for argument 'points1'\n>  - points1 data type = 17 is not supported\n>  - Expected Ptr<cv::UMat> for argument 'points1'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [17], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m pts1  \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(pts1)\n\u001b[1;32m     12\u001b[0m pts2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(pts2)\n\u001b[0;32m---> 15\u001b[0m F, mask \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfindFundamentalMat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpts1\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpts2\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFM_8POINT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx2.T @ F @ x1 = (should be close to 0)\u001b[39m\u001b[38;5;124m\"\u001b[39m )\n\u001b[1;32m     19\u001b[0m test_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.6.0) :-1: error: (-5:Bad argument) in function 'findFundamentalMat'\n> Overload resolution failed:\n>  - findFundamentalMat() missing required argument 'ransacReprojThreshold' (pos 4)\n>  - findFundamentalMat() missing required argument 'ransacReprojThreshold' (pos 4)\n>  - points1 data type = 17 is not supported\n>  - Expected Ptr<cv::UMat> for argument 'points1'\n>  - points1 data type = 17 is not supported\n>  - Expected Ptr<cv::UMat> for argument 'points1'\n"
     ]
    }
   ],
   "source": [
    "#print(np.array(kp1))\n",
    "\n",
    "# extract points\n",
    "pts1 = []\n",
    "pts2 = []\n",
    "for i,(m) in enumerate(matches):\n",
    "    #print(m.distance)\n",
    "    pts2.append(kp2[m[0].trainIdx])\n",
    "    pts1.append(kp1[m[0].queryIdx])\n",
    "\n",
    "pts1  = np.asarray(pts1)\n",
    "pts2 = np.asarray(pts2)\n",
    "\n",
    "\n",
    "F, mask = cv2.findFundamentalMat(pts1[:100], pts2[:100], cv2.FM_8POINT)\n",
    "print(\"x2.T @ F @ x1 = (should be close to 0)\" )\n",
    "\n",
    "\n",
    "test_idx = 20\n",
    "\n",
    "x1 = np.expand_dims(pts1[test_idx,:], axis=1)\n",
    "x2 = np.expand_dims(pts2[test_idx,:], axis=1)\n",
    "\n",
    "\n",
    "#print(np.shape(x2))\n",
    "\n",
    "\n",
    "\n",
    "print(x2.T @ F @ x1)\n",
    "\n",
    "E = K.T @ F @ K # Get the essential matrix\n",
    "\n",
    "[U,S,V] = np.linalg.svd(E)\n",
    "\n",
    "diag_110 = np.array([[1, 0, 0],[0, 1, 0],[0, 0, 0]])\n",
    "newE = U@diag_110@V\n",
    "[U,S,V] = np.linalg.svd(newE); # Perform second decompose to get S=diag(1,1,0)\n",
    "\n",
    "W = np.array([[0, -1, 0],[1, 0, 0],[0, 0, 1]]) # [0 -1 0; 1 0 0; 0 0 1];\n",
    "\n",
    "R1 = U@W@V\n",
    "R2 = U@W.T@V\n",
    "t1 = U[:,2] #norm = 1\n",
    "t2 = -U[:,2] #norm = 1\n",
    "\n",
    "\n",
    "#print(R1, t1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T_norm = np.diag([2/(1920 - 1.), 2/(1080 - 1.), 1]) # normalize\n",
    "T_norm[:,2] = [-1, -1, 1] # translate\n",
    "\n",
    "a = np.array([1920,1080,1]) @ np.linalg.inv(K.T)\n",
    "\n",
    "np.shape(np.squeeze(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[211, 149, 231, 226, 176, 131, 171, 133]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[297, 227, 276, 206, 201, 275, 152, 101]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print([m[0].queryIdx for m in best8])\n",
    "[m[0].trainIdx for m in best8]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "df0893f56f349688326838aaeea0de204df53a132722cbd565e54b24a8fec5f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
