{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pangolin\n",
    "import g2o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import os \n",
    "from pathlib import Path\n",
    "from numbers import Number\n",
    "import re\n",
    "import g2o\n",
    "from PIL import Image\n",
    "import g2o\n",
    "\n",
    "\n",
    "def KeyPoints2CameraCoords(kp, K, w, h):\n",
    "    # Keypoints to homogeneous point coordinates\n",
    "    kp_tf = np.concatenate((kp, np.ones((np.shape(kp)[0],1))), axis=1)\n",
    "    # Keypoints to camera coordinate system\n",
    "    kp_tf = kp_tf @ np.linalg.inv(K.T)\n",
    "    limits = np.squeeze(np.array([w, h, 1]) @ np.linalg.inv(K.T)) # e.g. [1920,1080,1] to camera coords\n",
    "    w_k, h_k = limits[0,0], limits[0,1]\n",
    "    # Define T_norm based on width and height of the image (now in camera coords)\n",
    "    T_norm = np.diag([2/(w_k - 1.), 2/(h_k - 1.), 1]) # normalize\n",
    "    T_norm[:,2] = [-1, -1, 1] # translate\n",
    "    # Conditioning/Normalization of keypoints\n",
    "    kp_tf = kp_tf @ T_norm.T\n",
    "    return kp_tf, T_norm\n",
    "\n",
    "\n",
    "def NormalizeKeypoints(kp, w, h):\n",
    "    # Keypoints to homogeneous point coordinates\n",
    "    kp_tf = np.concatenate((kp, np.ones((np.shape(kp)[0],1))), axis=1)\n",
    "    # Define T_norm based on width and height of the image (now in camera coords)\n",
    "    T_norm = np.diag([2/(w - 1.), 2/(h - 1.), 1]) # normalize\n",
    "    T_norm[:,2] = [-1, -1, 1] # translate\n",
    "    # Conditioning/Normalization of keypoints\n",
    "    kp_tf = kp_tf @ T_norm.T\n",
    "    return kp_tf, T_norm\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "class FeatureExtractor:\n",
    "    def __init__(self):\n",
    "        self.extractor = cv2.SIFT_create()\n",
    "        \n",
    "    def compute_features(self, img):\n",
    "        pts = cv2.goodFeaturesToTrack(np.mean(img, axis=2).astype(np.uint8), 3000, qualityLevel=0.01, minDistance=7)\n",
    "        kps = [cv2.KeyPoint(x=f[0][0], y=f[0][1], size=20) for f in pts]\n",
    "        kp, des = self.extractor.compute(img, kps)\n",
    "        return kp, des\n",
    "        \n",
    "        #kp, des = self.extractor.detectAndCompute(img,None)\n",
    "        #return kp, des\n",
    "\n",
    "\n",
    "class FeatureMatcher:\n",
    "    def __init__(self):\n",
    "        self.matcher = cv2.BFMatcher()\n",
    "    def match_features(self, frame_prev, frame_cur):\n",
    "        kp1, desc1 = frame_prev.keypoints, frame_prev.features\n",
    "        kp2, desc2 = frame_cur.keypoints, frame_cur.features\n",
    "        # Match descriptors.\n",
    "        matches = self.matcher.knnMatch(desc1,desc2,k=1)\n",
    "        # Sort the matches according to nearest neighbor distance ratio (NNDR) (CV course, exercise 4)\n",
    "        distmat = np.dot(desc1, desc2.T)\n",
    "        X_terms = np.expand_dims(np.diag(np.dot(desc1, desc1.T)), axis=1)\n",
    "        X_terms = np.tile(X_terms,(1,desc2.shape[0]))\n",
    "        Y_terms = np.expand_dims(np.diag(np.dot(desc2, desc2.T)), axis=0)\n",
    "        Y_terms = np.tile(Y_terms,(desc1.shape[0],1))\n",
    "        distmat = np.sqrt(Y_terms + X_terms - 2*distmat)\n",
    "        ## We determine the mutually nearest neighbors\n",
    "        dist1 = np.amin(distmat, axis=1)\n",
    "        ids1 = np.argmin(distmat, axis=1)\n",
    "        dist2 = np.amin(distmat, axis=0)\n",
    "        ids2 = np.argmin(distmat, axis=0)\n",
    "        pairs = []\n",
    "        for k in range(ids1.size):\n",
    "            if k == ids2[ids1[k]]:\n",
    "                pairs.append(np.array([k, ids1[k], dist1[k]]))\n",
    "        pairs = np.array(pairs)\n",
    "        # We sort the mutually nearest neighbors based on the nearest neighbor distance ratio\n",
    "        NNDR = []\n",
    "        for k,ids1_k,dist1_k in pairs:\n",
    "            r_k = np.sort(distmat[int(k),:])\n",
    "            nndr = r_k[0]/r_k[1]\n",
    "            NNDR.append(nndr)\n",
    "\n",
    "        id_nnd = np.argsort(NNDR)\n",
    "        return np.array(matches)[id_nnd]\n",
    "\n",
    "class TransformationEstimator:\n",
    "    # Uses ransac algorithm to find best estimation of essential matrix between images\n",
    "    def __init__(self, N=10, Threshold=0.2):\n",
    "        self.N_iterations = N\n",
    "        self.Threshold = Threshold\n",
    "        \n",
    "    def F_from_point_pairs(xs,xss):\n",
    "        # xs, xss: Nx3 homologous point coordinates, N>7\n",
    "        # returns F: 3x3 Fundamental matrix\n",
    "        # Coefficient matrix\n",
    "        N = np.size(xs)[0]\n",
    "        A = np.zeros((N, 9))\n",
    "        for n in range(N):\n",
    "            A[n,:] = np.kron(xss[n,:], xs[n,:])\n",
    "        # Singular-value-decomposition\n",
    "        U,D,V = np.linalg.svd(A, full_matrices=True, compute_uv=True)\n",
    "        Fa = np.reshape(V[:,:,8], (3,3)) # approximation of F, could be > rank 2\n",
    "        Ua,Da,Va = np.linalg.svd(Fa, full_matrices=True, compute_uv=True)\n",
    "        F = (Ua * np.diag([Da[0,0], Da[1,1], 0])) @ Va\n",
    "        return F \n",
    "    \n",
    "    def E_from_point_pairs(xs,xss):\n",
    "        # xs, xss: Nx3 homologous point coordinates, N>7\n",
    "        # returns E: 3x3 Essential matrix\n",
    "        # Coefficient matrix\n",
    "        N = np.size(xs)[0]\n",
    "        A = np.zeros((N, 9))\n",
    "        for n in range(N):\n",
    "            A[n,:] = np.kron(xss[n,:], xs[n,:])\n",
    "        # Singular-value-decomposition\n",
    "        U,D,V = np.linalg.svd(A, full_matrices=True, compute_uv=True)\n",
    "        Ea = np.reshape(V[:,:,8], (3,3)) # approximation of F, could be > rank 2\n",
    "        Ua,Da,Va = np.linalg.svd(Ea, full_matrices=True, compute_uv=True)\n",
    "        E = (Ua * np.diag([1,1,0])) @ Va\n",
    "        return E\n",
    "      \n",
    "    def ransack(self, cur_frame_kp, prev_frame_kp, matches, T_norm, K):\n",
    "        # cur_frame_kp, prev_frame_kp: Nx2 pixel coordinates of keypoints matching between images\n",
    "        # T_norm, 3x3 transformation matrix of keypoints: center of mass to (0,0), x and y to scale [-1,1]\n",
    "        # K: camera calibration matrix\n",
    "        N = np.size(cur_frame_kp)[0]\n",
    "        highest_number_of_inliers = -1\n",
    "        best_essential = np.eye(3)\n",
    "        seed_n = 8 # has to be > 7\n",
    "        # Keypoints to homogeneous point coordinates\n",
    "        kp1 = np.concatenate((prev_frame_kp, np.ones(np.shape(N))))\n",
    "        kp2 = np.concatenate((cur_frame_kp, np.ones(np.shape(N))))\n",
    "        # Keypoints to camera coordinate system\n",
    "        kp1 = kp1 @ K.T\n",
    "        kp2 = kp2 @ K.T\n",
    "        # Conditioning/Normalization of keypoints\n",
    "        kp1 = kp1 @ T_norm.T\n",
    "        kp2 = kp2 @ T_norm.T\n",
    "        # Ransac loop\n",
    "        for n in range(self.N_iterations):\n",
    "            # Randomly select a seed group of > 7 matches\n",
    "            seed_group = np.random.randint(low=0, high=N, size=seed_n)\n",
    "            xs = kp1[seed_group,:]\n",
    "            xss = kp2[seed_group,:]\n",
    "            E = self.E_from_point_pairs(xs, xss)\n",
    "        \n",
    "        return(E)\n",
    "\n",
    "class Frame:\n",
    "    def __init__(self, rgb_fp, d_path, feature_extractor):\n",
    "        self.rgb = cv2.imread(rgb_fp)\n",
    "        # depth file read is handled bit differently\n",
    "        depth = cv2.imread(d_path)\n",
    "        self.depth =  Image.open(d_path)\n",
    "        self.keypoints, self.features  = None, None\n",
    "        self.feature_extractor = feature_extractor\n",
    "    def process_frame(self):\n",
    "        self.keypoints, self.features = self.feature_extract(self.rgb)\n",
    "        return self.keypoints, self.features, self.rgb\n",
    "        \n",
    "    def feature_extract(self, rgb):\n",
    "        return self.feature_extractor.compute_features(rgb)\n",
    "        \n",
    "def compute_fundamental_matrix(kp1, kp2, matches):\n",
    "    \"\"\"\n",
    "    Takes in filenames of two input images \n",
    "    Return Fundamental matrix computes \n",
    "    using 8 point algorithm\n",
    "    \"\"\"\n",
    "    \n",
    "    # extract points\n",
    "    pts1 = []\n",
    "    pts2 = []\n",
    "    for i,(m) in enumerate(matches):\n",
    "        #print(m.distance)\n",
    "        pts2.append(kp2[m[0].trainIdx].pt)\n",
    "        pts1.append(kp1[m[0].queryIdx].pt)\n",
    "    pts1  = np.asarray(pts1)\n",
    "    pts2 = np.asarray(pts2)\n",
    "    \n",
    "    # Compute fundamental matrix\n",
    "    F, mask = cv2.findFundamentalMat(pts1,pts2,cv2.FM_8POINT)\n",
    "    return F \n",
    "\n",
    "def essentialToRt(E, opt = 1):\n",
    "    # see wikipedia: https://en.wikipedia.org/wiki/Essential_matrix\n",
    "    U,d,Vt = np.linalg.svd(E)\n",
    "    W = np.mat([[0, -1, 0], [1, 0, 0], [0, 0, 1]])\n",
    "    Winv = W.T\n",
    "    # ansatz (educated guess)\n",
    "    #t = U @ W @ np.diag(d) @ U.T\n",
    "    #t = Vt[-1,:]\n",
    "    R = U @ Winv @ Vt\n",
    "    Z = np.mat([[0, 1, 0], [-1, 0, 0], [0, 0, 0]]) \n",
    "    if opt==1:\n",
    "        t = U @ Z @ U.T\n",
    "        t = np.array([t[2,1], t[0,2], t[1,0]]) # get vector from transpose operation\n",
    "    elif opt==2:\n",
    "        t = U @ W @ np.diag(d) @ U.T\n",
    "        t = np.array([t[2,1], t[0,2], t[1,0]]) \n",
    "    else:\n",
    "        t = Vt[-1,:]\n",
    "    return R, t\n",
    "\n",
    "debug = True\n",
    "scale = 5000\n",
    "D = np.array([0, 0, 0, 0], dtype=np.float32)  # no distortion\n",
    "K = np.matrix([[481.20, 0, 319.5], [0, 480.0, 239.5], [0, 0, 1]])  # camera intrinsic parameters\n",
    "fx, fy, cx, cy = 481.20, 480.0, 319.5, 239.5\n",
    "\n",
    "width, height = 640, 480\n",
    "\n",
    "def get_color(img, pt):\n",
    "        x = int(np.clip(pt[0], 0, width - 1))\n",
    "        y = int(np.clip(pt[1], 0, height - 1))\n",
    "        color = img[y, x]\n",
    "        if isinstance(color, Number):\n",
    "            color = np.array([color, color, color])\n",
    "        return color[::-1] / 255.\n",
    "\n",
    "def point2dTo3d(n, m, d):\n",
    "    z = float(d) / scale\n",
    "    x = (n - cx) * z / fx\n",
    "    y = (m - cy) * z / fy\n",
    "    point = np.array([x, y, z], dtype=np.float32)\n",
    "    return point\n",
    "\n",
    "\n",
    "def solvePnP(frame1, frame2, matches):\n",
    "        kp1, kp2, des1, des2, depth = frame1.keypoints, frame2.keypoints, frame1.features, frame2.features, frame1.depth\n",
    "        goodMatches = matches\n",
    "    \n",
    "        pts_obj, pts_img2, pts_img1 = [], [], []\n",
    "        colour = []\n",
    "        features = []\n",
    "        for i in range(0, len(goodMatches)):\n",
    "            p = kp1[goodMatches[i][0].queryIdx].pt\n",
    "            # d = depth[int(p[1])][int(p[0])]\n",
    "            d = depth.getpixel((int(p[0]), int(p[1])))\n",
    "            if d == 0:\n",
    "                pass\n",
    "            else:\n",
    "                p2 = kp2[goodMatches[i][0].trainIdx].pt\n",
    "                #dif = abs(cv2.norm(p) - cv2.norm(p2))\n",
    "                #if dif > .1:\n",
    "                    #print('dif -> {}'.format(dif))\n",
    "                    #pass\n",
    "                pts_img2.append(p2)\n",
    "                pts_img1.append(p)\n",
    "                pd = point2dTo3d(p[0], p[1], d)\n",
    "                pts_obj.append(pd)\n",
    "                # c = frame1.rgb[int(p[1])][int(p[0])]\n",
    "                colour.append(get_color(img=frame1.rgb, pt=p))\n",
    "                features.append(des1[goodMatches[i][0].queryIdx])\n",
    "\n",
    "        pts_obj, pts_img2 = np.array(pts_obj), np.array(pts_img2)\n",
    "        pts_img1 = np.array(pts_img1)\n",
    "        if debug:\n",
    "            print('pts_obj -> {}, pts_img->{}'.format(np.shape(pts_obj), np.shape(pts_img2)))\n",
    "            print(np.shape(cv2.solvePnPRansac(pts_obj, pts_img2, K, D, useExtrinsicGuess=False)))\n",
    "        \n",
    "        retval, rvec, tvec, inliers = cv2.solvePnPRansac(pts_obj, pts_img2, K, D, useExtrinsicGuess=False)\n",
    "        return retval, rvec, tvec, inliers\n",
    "\n",
    "def transformMatrix(rvec, tvec):\n",
    "        r, t = np.matrix(rvec), np.matrix(tvec)\n",
    "        R, _ = cv2.Rodrigues(r)\n",
    "        Rt = np.hstack((R, t))\n",
    "        T = np.vstack((Rt, np.matrix([0, 0, 0, 1])))\n",
    "        return T\n",
    "\n",
    "\n",
    "def opencv_R_t(E,kp1, kp2, matches):\n",
    "    \"\"\" \n",
    "    Return Fundamental matrix computes \n",
    "    using 8 point algorithm\n",
    "    \"\"\"\n",
    "    \n",
    "    # extract points\n",
    "    pts1 = []\n",
    "    pts2 = []\n",
    "    for i,(m) in enumerate(matches):\n",
    "        #print(m.distance)\n",
    "        pts2.append(kp2[m[0].trainIdx].pt)\n",
    "        pts1.append(kp1[m[0].queryIdx].pt)\n",
    "    pts1  = np.asarray(pts1)\n",
    "    pts2 = np.asarray(pts2)\n",
    "    \n",
    "    pts_l_norm = cv2.undistortPoints(np.expand_dims(pts1, axis=1), cameraMatrix=K, distCoeffs=None)\n",
    "    pts_r_norm = cv2.undistortPoints(np.expand_dims(pts2, axis=1), cameraMatrix=K, distCoeffs=None)\n",
    "\n",
    "    points, R, t, mask = cv2.recoverPose(E,pts_l_norm, pts_r_norm)\n",
    "    return R,t\n",
    "\n",
    "\n",
    "def triangulation(kp1, kp2, T_1w, T_2w):\n",
    "    \"\"\"Triangulation to get 3D points\n",
    "    Args:\n",
    "        kp1 (Nx2): keypoint in view 1 (normalized)\n",
    "        kp2 (Nx2): keypoints in view 2 (normalized)\n",
    "        T_1w (4x4): pose of view 1 w.r.t  i.e. T_1w (from w to 1)\n",
    "        T_2w (4x4): pose of view 2 w.r.t world, i.e. T_2w (from w to 2)\n",
    "    Returns:\n",
    "        X (3xN): 3D coordinates of the keypoints w.r.t world coordinate\n",
    "        X1 (3xN): 3D coordinates of the keypoints w.r.t view1 coordinate\n",
    "        X2 (3xN): 3D coordinates of the keypoints w.r.t view2 coordinate\n",
    "    \"\"\"\n",
    "    kp1_3D = np.ones((3, kp1.shape[0]))\n",
    "    kp2_3D = np.ones((3, kp2.shape[0]))\n",
    "    kp1_3D[0], kp1_3D[1] = kp1[:, 0].copy(), kp1[:, 1].copy()\n",
    "    kp2_3D[0], kp2_3D[1] = kp2[:, 0].copy(), kp2[:, 1].copy()\n",
    "    X = cv2.triangulatePoints(T_1w[:3], T_2w[:3], kp1_3D[:2], kp2_3D[:2])\n",
    "    X /= X[3]\n",
    "    X1 = T_1w[:3] @ X\n",
    "    X2 = T_2w[:3] @ X\n",
    "    return X[:3], X1, X2 \n",
    "\n",
    "\n",
    "def estimate_relative_pose_from_correspondence(pts1, pts2, K1, K2):\n",
    "        f_avg = (K1[0, 0] + K2[0, 0]) / 2\n",
    "        pts1, pts2 = np.ascontiguousarray(pts1, np.float32), np.ascontiguousarray(pts2, np.float32)\n",
    "\n",
    "        pts_l_norm = cv2.undistortPoints(np.expand_dims(pts1, axis=1), cameraMatrix=K1, distCoeffs=None)\n",
    "        pts_r_norm = cv2.undistortPoints(np.expand_dims(pts2, axis=1), cameraMatrix=K2, distCoeffs=None)\n",
    "\n",
    "        E, mask = cv2.findEssentialMat(pts_l_norm, pts_r_norm, focal=1.0, pp=(0., 0.),\n",
    "                                       method=cv2.RANSAC, prob=0.999, threshold=3.0 / f_avg)\n",
    "        points, R_est, t_est, mask_pose = cv2.recoverPose(E, pts_l_norm, pts_r_norm)\n",
    "        return mask[:,0].astype(np.bool), R_est, t_est \n",
    "\n",
    "\n",
    "def estimateEssential(kp1, kp2, matches, K, essTh = K[0,0]):\n",
    "    #f_avg = K[0,0]\n",
    "    # match keypoints\n",
    "    pts1 = []\n",
    "    pts2 = []\n",
    "    for i,(m) in enumerate(matches):\n",
    "        #print(m.distance)\n",
    "        pts2.append(kp2[m[0].trainIdx].pt)\n",
    "        pts1.append(kp1[m[0].queryIdx].pt)\n",
    "    pts1  = np.asarray(pts1)\n",
    "    pts2 = np.asarray(pts2)\n",
    "    # normalize points\n",
    "    pts_l_norm = cv2.undistortPoints(np.expand_dims(pts1, axis=1), cameraMatrix=K, distCoeffs=None)\n",
    "    pts_r_norm = cv2.undistortPoints(np.expand_dims(pts2, axis=1), cameraMatrix=K, distCoeffs=None)\n",
    "    E, inliers = cv2.findEssentialMat(pts_l_norm, pts_r_norm, focal=1.0, pp=(0., 0.),\n",
    "                                    method=cv2.RANSAC, prob=0.999, threshold=3.0 / essTh)\n",
    "    return E, inliers\n",
    "        \n",
    "\n",
    "\n",
    "def estimateHomography(kp1, kp2, matches, K, homTh=K[0,0]):\n",
    "    # match keypoints\n",
    "    pts1 = []\n",
    "    pts2 = []\n",
    "    for i,(m) in enumerate(matches):\n",
    "        #print(m.distance)\n",
    "        pts2.append(kp2[m[0].trainIdx].pt)\n",
    "        pts1.append(kp1[m[0].queryIdx].pt)\n",
    "    pts1  = np.asarray(pts1)\n",
    "    pts2 = np.asarray(pts2)\n",
    "    # normalize points\n",
    "    pts_l_norm = cv2.undistortPoints(np.expand_dims(pts1, axis=1), cameraMatrix=K, distCoeffs=None)\n",
    "    pts_r_norm = cv2.undistortPoints(np.expand_dims(pts2, axis=1), cameraMatrix=K, distCoeffs=None)\n",
    "    \n",
    "    H, inliers = cv2.findHomography(pts_l_norm, pts_r_norm, cv2.RANSAC, ransacReprojThreshold=homTh)\n",
    "    return H, inliers\n",
    "\n",
    "\n",
    "def estimateRelativePose(tform, kp1, kp2, matches, K, tform_type = \"Homography\"):\n",
    "    pts1 = []\n",
    "    pts2 = []\n",
    "    for i,(m) in enumerate(matches):\n",
    "        #print(m.distance)\n",
    "        pts2.append(kp2[m[0].trainIdx].pt)\n",
    "        pts1.append(kp1[m[0].queryIdx].pt)\n",
    "    pts1  = np.asarray(pts1)\n",
    "    pts2 = np.asarray(pts2)\n",
    "    # normalize points\n",
    "    pts_l_norm = cv2.undistortPoints(np.expand_dims(pts1, axis=1), cameraMatrix=K, distCoeffs=None)\n",
    "    pts_r_norm = cv2.undistortPoints(np.expand_dims(pts2, axis=1), cameraMatrix=K, distCoeffs=None)\n",
    "    \n",
    "    if tform_type == \"Homography\":\n",
    "        num, Rs, Ts, Ns  = cv2.decomposeHomographyMat(tform, K)\n",
    "        R, t, validFraction = chooseRealizableSolution(Rs, Ts, K, pts_l_norm, pts_r_norm)\n",
    "        return R, t\n",
    "        \n",
    "    elif tform_type == \"Essential\":\n",
    "        points, R, t, _ = cv2.recoverPose(tform, pts_l_norm, pts_r_norm)    \n",
    "        return R, t\n",
    "    else:\n",
    "        print(\"Unknown transformation type\")\n",
    "        \n",
    "        \n",
    "import numpy as np\n",
    "\n",
    "# TODO: replace all \n",
    "\n",
    "def triangulateMidPoint(points1, points2, P1, P2):\n",
    "    points1 = np.squeeze(points1)\n",
    "    points2 = np.squeeze(points2)\n",
    "    numPoints = points1.shape[0]\n",
    "    points3D = np.zeros((numPoints,3))\n",
    "    P1 = P1.T\n",
    "    P2 = P2.T\n",
    "    M1 = P1[:3, :3]\n",
    "    M2 = P2[:3, :3]\n",
    "    # Get least-squares solution\n",
    "    c1 = np.linalg.lstsq(-M1,  P1[:,3], rcond=None)[0]\n",
    "    #print(np.shape(M1))\n",
    "    c2 = np.linalg.lstsq(-M2, P2[:,3], rcond=None)[0]\n",
    "    y = c2 - c1\n",
    "    u1 = np.concatenate((points1, np.ones((numPoints,1))), axis=1)\n",
    "    u2 = np.concatenate((points2, np.ones((numPoints,1))), axis=1)\n",
    "    #u1 = [points1, ones(numPoints, 1, 'like', points1)]'\n",
    "    #u2 = [points2, ones(numPoints, 1, 'like', points1)]'\n",
    "    a1 = np.linalg.lstsq(M1, u1.T, rcond=None)[0]\n",
    "    a2 = np.linalg.lstsq(M2, u2.T, rcond=None)[0]\n",
    "    #isCodegen  = ~isempty(coder.target);\n",
    "    condThresh = 2**(-52)\n",
    "    for i in range(numPoints):\n",
    "        A   = np.array([a1[:,i], -a2[:,i]]).T \n",
    "        AtA = A.T@A\n",
    "        if np.linalg.cond(AtA) < condThresh: # original: rcond(AtA) < condThresh\n",
    "            # Guard against matrix being singular or ill-conditioned\n",
    "            p    = np.inf(3, 1)\n",
    "            p[2] = -p[2]\n",
    "        else:\n",
    "            alpha = np.linalg.lstsq(A, y, rcond=None)[0]\n",
    "            p = (c1 + (alpha[0] * a1[:,i]).T + c2 + (alpha[1] * a2[:,i]).T) / 2\n",
    "            \n",
    "        points3D[i, :] = p.T\n",
    "\n",
    "    return points3D\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def chooseRealizableSolution(Rs, Ts, K, points1, points2):\n",
    "    # Rs is 4x3x3, holding all possible solutions of Rotation matrix\n",
    "    # Ts is 4x3x1, holding all possible solutions of Translation vector\n",
    "    numNegatives = np.zeros((np.shape(Ts)[0], 1))\n",
    "    #  The camera matrix is computed as follows:\n",
    "    #  camMatrix = [rotationMatrix; translationVector] * K\n",
    "    #  where K is the intrinsic matrix.\n",
    "    #camMatrix1 = cameraMatrix(cameraParams1, np.eye(3), np.array([0 0 0]));\n",
    "    camMatrix1 = np.concatenate((np.eye(3), np.zeros((1,3))), axis=0) @ K\n",
    "    \n",
    "    for i in range(np.shape(Ts)[0]):\n",
    "        #camMatrix2 = cameraMatrix(cameraParams2, Rs(:,:,i)', Ts(i, :));\n",
    "        #camMatrix2 is 4x3 @ 3x3 matmul\n",
    "        camMatrix2 = np.concatenate((Rs[i].T, Ts[i].T),axis=0) @ K\n",
    "        m1 = triangulateMidPoint(points1, points2, camMatrix1, camMatrix2)\n",
    "        #m2 = bsxfun(@plus, m1 * Rs(:,:,i)', Ts(i, :));\n",
    "        m2 = (m1 @ (Rs[i]).T) + Ts[i].T\n",
    "        numNegatives[i] = np.sum((m1[:,2] < 0) | (m2[:,2] < 0))\n",
    "        \n",
    "    val = np.min(numNegatives)\n",
    "    idx = np.where(numNegatives==val)\n",
    "    validFraction = 1 - (val / points1.shape[0])\n",
    "    \n",
    "    R = np.zeros((len(idx), 3,3))\n",
    "    t = np.zeros((len(idx), 3))\n",
    "    for n in range(len(idx)):\n",
    "        idx_n = idx[n][0]\n",
    "        R0 = Rs[idx_n].T\n",
    "        t0 = Ts[idx_n].T\n",
    "\n",
    "        tNorm = np.linalg.norm(t0)\n",
    "        if tNorm != 0:\n",
    "            t0 = t0 / tNorm\n",
    "        R[n] = R0\n",
    "        t[n] = t0\n",
    "\n",
    "    return R, t, validFraction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chose Essential\n",
      "Rotation\n",
      "[[ 9.99504922e-01 -1.67506360e-02 -2.66331853e-02]\n",
      " [ 1.67488668e-02  9.99859686e-01 -2.89518124e-04]\n",
      " [ 2.66342979e-02 -1.56700885e-04  9.99645232e-01]]\n",
      "Translation\n",
      "[[0.31807891]\n",
      " [0.10384802]\n",
      " [0.94235948]]\n",
      "chose Homography\n",
      "Rotation\n",
      "[[[ 0.20331852 -0.92008714  0.33481508]\n",
      "  [-0.91678056 -0.29895901 -0.26483298]\n",
      "  [ 0.34376571 -0.2531063  -0.90430199]]\n",
      "\n",
      " [[ 0.20331852 -0.92008714  0.33481508]\n",
      "  [-0.91678056 -0.29895901 -0.26483298]\n",
      "  [ 0.34376571 -0.2531063  -0.90430199]]]\n",
      "Translation\n",
      "[[-0.51165926 -0.38660982  0.76729241]\n",
      " [ 0.51165926  0.38660982 -0.76729241]]\n",
      "chose Homography\n",
      "Rotation\n",
      "[[[-0.04083678 -0.76428572  0.25635332]\n",
      "  [-0.18941529 -0.57295569  0.08691766]\n",
      "  [ 0.54279238 -0.2949066  -0.78638341]]\n",
      "\n",
      " [[ 0.33133925 -0.37638697 -0.63270092]\n",
      "  [ 0.40450204 -0.27245962 -0.36552442]\n",
      "  [-0.24243779 -0.8851439   0.39716164]]]\n",
      "Translation\n",
      "[[-0.51095048 -0.38375825  0.76919387]\n",
      " [-0.51097986 -0.38375915  0.7691739 ]]\n",
      "chose Homography\n",
      "Rotation\n",
      "[[[-0.81236261 -0.55434541 -0.18101981]\n",
      "  [-0.56810797  0.68225476  0.46019754]\n",
      "  [-0.13160677  0.47668607 -0.8691663 ]]\n",
      "\n",
      " [[-0.81236261 -0.55434541 -0.18101981]\n",
      "  [-0.56810797  0.68225476  0.46019754]\n",
      "  [-0.13160677  0.47668607 -0.8691663 ]]]\n",
      "Translation\n",
      "[[-0.52147259 -0.38592223  0.76100616]\n",
      " [ 0.52147259  0.38592223 -0.76100616]]\n",
      "chose Homography\n",
      "Rotation\n",
      "[[[ 0.32285902  0.11405162  0.93954999]\n",
      "  [ 0.12289273 -0.98935963  0.07786856]\n",
      "  [ 0.93843448  0.0903238  -0.33343995]]\n",
      "\n",
      " [[ 0.32285902  0.11405162  0.93954999]\n",
      "  [ 0.12289273 -0.98935963  0.07786856]\n",
      "  [ 0.93843448  0.0903238  -0.33343995]]]\n",
      "Translation\n",
      "[[-0.51039523 -0.38722827  0.76782223]\n",
      " [ 0.51039523  0.38722827 -0.76782223]]\n",
      "chose Homography\n",
      "Rotation\n",
      "[[[-0.90032096 -0.41435813 -0.13316816]\n",
      "  [-0.41010694  0.70521084  0.57835108]\n",
      "  [-0.1457353   0.57531297 -0.80484668]]\n",
      "\n",
      " [[ 0.99997703 -0.00629016 -0.00326583]\n",
      "  [ 0.00630399  0.99997124  0.00421686]\n",
      "  [ 0.00324208 -0.00423731  0.99998673]]]\n",
      "Translation\n",
      "[[-0.50838299 -0.38370845  0.77091799]\n",
      " [-0.51109308 -0.38373731  0.76910958]]\n",
      "chose Homography\n",
      "Rotation\n",
      "[[[-0.96815461  0.20882839  0.1380831 ]\n",
      "  [ 0.22058188  0.45068281  0.86500206]\n",
      "  [ 0.11840553  0.86791453 -0.48239446]]\n",
      "\n",
      " [[ 0.99981149 -0.01728574 -0.00881819]\n",
      "  [ 0.01738692  0.99978256  0.01150911]\n",
      "  [ 0.00861714 -0.01166018  0.99989484]]]\n",
      "Translation\n",
      "[[-0.50612297 -0.38583416  0.77134398]\n",
      " [-0.51119609 -0.38365356  0.7690829 ]]\n",
      "chose Homography\n",
      "Rotation\n",
      "[[[ 0.45490839 -0.60159036  0.65661814]\n",
      "  [-0.5427114  -0.77186721 -0.33118778]\n",
      "  [ 0.70606139 -0.20569406 -0.67761882]]\n",
      "\n",
      " [[ 0.45490839 -0.60159036  0.65661814]\n",
      "  [-0.5427114  -0.77186721 -0.33118778]\n",
      "  [ 0.70606139 -0.20569406 -0.67761882]]]\n",
      "Translation\n",
      "[[-0.51534706 -0.41580215  0.74935037]\n",
      " [ 0.51534706  0.41580215 -0.74935037]]\n",
      "chose Homography\n",
      "Rotation\n",
      "[[[ 0.38546929 -0.02373027  0.92241547]\n",
      "  [ 0.02959392 -0.99883701 -0.03806334]\n",
      "  [ 0.92224597  0.04197014 -0.38431873]]\n",
      "\n",
      " [[ 0.38546929 -0.02373027  0.92241547]\n",
      "  [ 0.02959392 -0.99883701 -0.03806334]\n",
      "  [ 0.92224597  0.04197014 -0.38431873]]]\n",
      "Translation\n",
      "[[-0.50596927 -0.4065207   0.76074701]\n",
      " [ 0.50596927  0.4065207  -0.76074701]]\n",
      "chose Homography\n",
      "Rotation\n",
      "[[[ 0.18265569 -0.92774571  0.32546134]\n",
      "  [-0.93046324 -0.27005482 -0.24760889]\n",
      "  [ 0.31760926 -0.25760336 -0.91256006]]\n",
      "\n",
      " [[ 0.98925967 -0.12767731 -0.0711625 ]\n",
      "  [ 0.13339455  0.98763362  0.0823854 ]\n",
      "  [ 0.0597644  -0.09099203  0.99405716]]]\n",
      "Translation\n",
      "[[-0.50932995 -0.3815905   0.77134408]\n",
      " [-0.51107273 -0.38409155  0.76894625]]\n"
     ]
    }
   ],
   "source": [
    "# Filepaths\n",
    "cur_dir = \"/home/juuso\"\n",
    "dir_rgb = cur_dir + \"/visual_slam/data/ICL_NUIM/rgb/\"\n",
    "dir_depth = cur_dir + \"/visual_slam/data/ICL_NUIM/depth/\"\n",
    "is_WINDOWS = False\n",
    "if is_WINDOWS:\n",
    "    dir_rgb = dir_rgb.replace(\"/\", \"\\\\\")\n",
    "    dir_depth = dir_depth.replace(\"/\", \"\\\\\")\n",
    "# Initialize\n",
    "feature_extractor = FeatureExtractor()\n",
    "feature_matcher = FeatureMatcher()\n",
    "# run feature extraction for 1st image\n",
    "fp_rgb = dir_rgb + str(1) + \".png\"\n",
    "fp_depth = dir_depth + str(1) + \".png\"\n",
    "cur_frame = Frame(fp_rgb, fp_depth, feature_extractor)\n",
    "kp, features, rgb = cur_frame.process_frame() \n",
    "prev_frame = cur_frame\n",
    "\n",
    "for i in range(2,1000):\n",
    "    if i % 30 == 0:\n",
    "        fp_rgb = dir_rgb + str(i) + \".png\"\n",
    "        fp_depth = dir_depth + str(i) + \".png\"\n",
    "        # Feature Extraction for current frame\n",
    "        cur_frame = Frame(fp_rgb, fp_depth, feature_extractor)\n",
    "        kp, features, rgb = cur_frame.process_frame()\n",
    "        # Feature Matching to previous frame\n",
    "        matches = feature_matcher.match_features(prev_frame, cur_frame)    \n",
    "        # if not enough matches (<100) continue to next frame\n",
    "        if(len(matches) < 100):\n",
    "            continue\n",
    "        # TODO: compute homography and inliers\n",
    "        H, inliersH  = estimateHomography(prev_frame.keypoints, cur_frame.keypoints, matches, K)\n",
    "        # TODO: compute essential and inliers\n",
    "        E, inliersE  = estimateEssential(prev_frame.keypoints, cur_frame.keypoints, matches, K)\n",
    "        # TODO: choose between models based on number of inliers\n",
    "        # https://www.programcreek.com/python/example/70413/cv2.RANSAC\n",
    "        tform = H\n",
    "        inliers = inliersH\n",
    "        tform_type = \"Homography\"\n",
    "        if sum(inliersH) < sum(inliersE):\n",
    "            print(\"Chose Essential\")\n",
    "            tform = E\n",
    "            inliers = inliersE\n",
    "            tform_type = \"Essential\"\n",
    "        else:\n",
    "            print(\"chose Homography\")\n",
    "        # TODO: if number of inliers with the better model too low continue to next frame\n",
    "        if sum(inliers) < 100:\n",
    "            continue\n",
    "        # TODO: get pose transformation\n",
    "        r,t = estimateRelativePose(tform, prev_frame.keypoints, cur_frame.keypoints, matches, K, tform_type)\n",
    "        print(\"Rotation\")\n",
    "        print(r)\n",
    "        print(\"Translation\")\n",
    "        print(t)\n",
    "        # TODO: triangulate two view to obtain 3-D map points\n",
    "        \n",
    "        \n",
    "        # Display\n",
    "        img3 = cv2.drawMatchesKnn(prev_frame.rgb,prev_frame.keypoints, cur_frame.rgb,cur_frame.keypoints,matches[:100],None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "        #img2 = cv2.drawKeypoints(rgb, kp, None, color=(0,255,0), flags=0)\n",
    "        cv2.imshow('a', img3)\n",
    "        cv2.waitKey(0)\n",
    "        #\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        prev_frame = cur_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filepaths\n",
    "cur_dir = \"/home/juuso\"\n",
    "dir_rgb = cur_dir + \"/visual_slam/data/ICL_NUIM/rgb/\"\n",
    "dir_depth = cur_dir + \"/visual_slam/data/ICL_NUIM/depth/\"\n",
    "is_WINDOWS = False\n",
    "if is_WINDOWS:\n",
    "    dir_rgb = dir_rgb.replace(\"/\", \"\\\\\")\n",
    "    dir_depth = dir_depth.replace(\"/\", \"\\\\\")\n",
    "# Initialize\n",
    "feature_extractor = FeatureExtractor()\n",
    "feature_matcher = FeatureMatcher()\n",
    "\n",
    "\n",
    "# run feature extraction for 1st image\n",
    "fp_rgb = dir_rgb + str(1) + \".png\"\n",
    "fp_depth = dir_depth + str(1) + \".png\"\n",
    "cur_frame = Frame(fp_rgb, fp_depth, feature_extractor)\n",
    "kp1, features1, rgb1 = cur_frame.process_frame() \n",
    "\n",
    "prev_frame = cur_frame\n",
    "i = 30 # jump to 30th frame\n",
    "\n",
    "fp_rgb = dir_rgb + str(i) + \".png\"\n",
    "fp_depth = dir_depth + str(i) + \".png\"\n",
    "# Feature Extraction for current frame\n",
    "cur_frame = Frame(fp_rgb, fp_depth, feature_extractor)\n",
    "kp2, features2, rgb2 = cur_frame.process_frame()\n",
    "\n",
    "# Feature Matching to previous frame\n",
    "matches = feature_matcher.match_features(prev_frame, cur_frame) \n",
    "\n",
    "best8 = matches[:8]\n",
    "\n",
    "F = compute_fundamental_matrix(kp1, kp2, matches[:100])\n",
    "F\n",
    "#img3 = cv2.drawMatchesKnn(prev_frame.rgb,prev_frame.keypoints, cur_frame.rgb,cur_frame.keypoints,matches[:8],None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "#img2 = cv2.drawKeypoints(rgb, kp, None, color=(0,255,0), flags=0)\n",
    "#cv2.imshow('a', img3)\n",
    "#cv2.waitKey(0)\n",
    "E = K.T @ F @ K\n",
    "#print(E)\n",
    "\n",
    "print(\"Handcrafted\")\n",
    "R, t = essentialToRt(E)\n",
    "t = np.array([t[2,1], t[0,2], t[1,0]])\n",
    "print(R)\n",
    "print(t)\n",
    "\n",
    "print(np.linalg.norm(t))\n",
    "\n",
    "\n",
    "print(\"opencv\")\n",
    "\n",
    "R, t = opencv_R_t(E, kp1, kp2, matches[:100])\n",
    "print(R)\n",
    "print(t)\n",
    "\n",
    "\n",
    "#t = np.array([t_mat[2,1], t_mat[0,2], t_mat[1,0]])\n",
    "#t = t/np.linalg.norm(t)\n",
    "#print(np.linalg.norm(t))\n",
    "# E = U@np.diag(S)@VT\n",
    "\n",
    "\n",
    "print(\"Ground truth\")\n",
    "retval, rvec, tvec, inliers = solvePnP(prev_frame, cur_frame, matches)\n",
    "T = transformMatrix(rvec, tvec)\n",
    "print(T)\n",
    "print(np.linalg.norm(tvec))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(np.array(kp1))\n",
    "\n",
    "# extract points\n",
    "pts1 = []\n",
    "pts2 = []\n",
    "for i,(m) in enumerate(matches):\n",
    "    #print(m.distance)\n",
    "    pts2.append(kp2[m[0].trainIdx])\n",
    "    pts1.append(kp1[m[0].queryIdx])\n",
    "\n",
    "pts1  = np.asarray(pts1)\n",
    "pts2 = np.asarray(pts2)\n",
    "\n",
    "\n",
    "F, mask = cv2.findFundamentalMat(pts1[:100], pts2[:100], cv2.FM_8POINT)\n",
    "print(\"x2.T @ F @ x1 = (should be close to 0)\" )\n",
    "\n",
    "\n",
    "test_idx = 20\n",
    "\n",
    "x1 = np.expand_dims(pts1[test_idx,:], axis=1)\n",
    "x2 = np.expand_dims(pts2[test_idx,:], axis=1)\n",
    "\n",
    "\n",
    "#print(np.shape(x2))\n",
    "\n",
    "\n",
    "\n",
    "print(x2.T @ F @ x1)\n",
    "\n",
    "E = K.T @ F @ K # Get the essential matrix\n",
    "\n",
    "[U,S,V] = np.linalg.svd(E)\n",
    "\n",
    "diag_110 = np.array([[1, 0, 0],[0, 1, 0],[0, 0, 0]])\n",
    "newE = U@diag_110@V\n",
    "[U,S,V] = np.linalg.svd(newE); # Perform second decompose to get S=diag(1,1,0)\n",
    "\n",
    "W = np.array([[0, -1, 0],[1, 0, 0],[0, 0, 1]]) # [0 -1 0; 1 0 0; 0 0 1];\n",
    "\n",
    "R1 = U@W@V\n",
    "R2 = U@W.T@V\n",
    "t1 = U[:,2] #norm = 1\n",
    "t2 = -U[:,2] #norm = 1\n",
    "\n",
    "\n",
    "#print(R1, t1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_norm = np.diag([2/(1920 - 1.), 2/(1080 - 1.), 1]) # normalize\n",
    "T_norm[:,2] = [-1, -1, 1] # translate\n",
    "\n",
    "a = np.array([1920,1080,1]) @ np.linalg.inv(K.T)\n",
    "\n",
    "np.shape(np.squeeze(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([m[0].queryIdx for m in best8])\n",
    "[m[0].trainIdx for m in best8]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
